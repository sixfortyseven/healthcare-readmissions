{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iFZzAMKdMyw01FzF0IksRJvl",
      "metadata": {
        "id": "iFZzAMKdMyw01FzF0IksRJvl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#! pip install kfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q217KKhFZq5N",
      "metadata": {
        "id": "q217KKhFZq5N"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "from kfp import compiler\n",
        "from kfp.dsl import pipeline, component, InputPath, OutputPath, Input, Output, Dataset, Artifact, Model, Metrics\n",
        "import joblib, gcsfs, fsspec\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iu8AOoYSZu3_",
      "metadata": {
        "id": "iu8AOoYSZu3_"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\",\"fsspec\", \"gcsfs\"])\n",
        "def loading_data(input_dataset_path:str, output_dataset_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.read_csv(input_dataset_path,keep_default_na=False,na_values=[\"\"])\n",
        "    df.to_csv(output_dataset_path, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DJjDDs8ReGy3",
      "metadata": {
        "id": "DJjDDs8ReGy3"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", 'numpy'])\n",
        "def preprocess_data(input_data_path: InputPath('Dataset'),\n",
        "                    output_data_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    df = pd.read_csv(input_data_path,keep_default_na=False,na_values=[\"\"])\n",
        "\n",
        "    # 1. Handling Missing Values\n",
        "    df['Number of Prior Visits'] = df['Number of Prior Visits'].fillna(df['Number of Prior Visits'].mode()[0])\n",
        "    df['Medications Prescribed'] = df['Medications Prescribed'].fillna(df['Medications Prescribed'].mode()[0])\n",
        "\n",
        "    # Remove age outliers\n",
        "    df = df[df['Age'] <= 100]\n",
        "\n",
        "    # 2. Feature Engineering\n",
        "    exercise_map = {'None': 0, 'Occasional': 1, 'Regular': 2}\n",
        "    df['Exercise_Encoded'] = df['Exercise Frequency'].map(exercise_map)\n",
        "\n",
        "    def bmi_category(bmi):\n",
        "        if bmi < 18.5:\n",
        "            return 'Underweight'\n",
        "        elif bmi < 25:\n",
        "            return 'Normal'\n",
        "        elif bmi < 30:\n",
        "            return 'Overweight'\n",
        "        else:\n",
        "            return 'Obese'\n",
        "    df['BMI_Category'] = df['BMI'].apply(bmi_category)\n",
        "\n",
        "    def age_group(age):\n",
        "        if age < 40:\n",
        "            return '<40'\n",
        "        elif age < 65:\n",
        "            return '40-64'\n",
        "        else:\n",
        "            return '65+'\n",
        "    df['Age_Group'] = df['Age'].apply(age_group)\n",
        "\n",
        "    # One-hot encode\n",
        "    df = pd.get_dummies(df, columns=[\n",
        "        'Gender', 'Ethnicity', 'Diet Type', 'Type of Treatment',\n",
        "        'BMI_Category', 'Age_Group'\n",
        "    ], drop_first=True)\n",
        "\n",
        "    # 3. skewed variable\n",
        "    df['LOS_Log'] = np.log1p(df['Length of Stay'])\n",
        "\n",
        "\n",
        "    # 4. Feature Selection\n",
        "    df = df.drop(columns=[\n",
        "        'Hospital ID', 'Adjusted Weight (kg)', 'Weight (kg)', 'Exercise Frequency', 'Length of Stay'\n",
        "    ])\n",
        "\n",
        "    df.to_csv(output_data_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5BKQMjwWZ6Ib",
      "metadata": {
        "id": "5BKQMjwWZ6Ib"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install = ['pandas', 'numpy', 'scikit-learn', 'joblib', 'fsspec', 'gcsfs'])\n",
        "def normalize_testing_data(dataset_path: InputPath('Dataset'),\n",
        "                           scaler_path:str,\n",
        "                            normalized_dataset_path: OutputPath('Dataset')\n",
        "                            ):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "    import gcsfs\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    if 'PatientID' not in df.columns:\n",
        "      raise ValueError('Testing data must contain PatientID column')\n",
        "    patient_ids = df['PatientID']\n",
        "    features = df.drop(columns=['PatientID'])\n",
        "\n",
        "    with gcsfs.GCSFileSystem().open(scaler_path, 'rb') as f:\n",
        "      scaler = joblib.load(f)\n",
        "\n",
        "    # Save the normalized dataset\n",
        "    df_scaled = pd.DataFrame(scaler.transform(features), columns=features.columns)\n",
        "    df_scaled['PatientID'] = patient_ids\n",
        "    df_scaled.to_csv(normalized_dataset_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vSUpdCELc6ZI",
      "metadata": {
        "id": "vSUpdCELc6ZI"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install = ['pandas', 'numpy', 'scikit-learn', 'joblib', 'fsspec', 'gcsfs'])\n",
        "def predicting_model(testing_data_path: InputPath('Dataset'), model_path:str, prediction_dataset_path:OutputPath('Dataset')):\n",
        "  import pandas as pd\n",
        "  import joblib\n",
        "  import gcsfs\n",
        "\n",
        "  test_data = pd.read_csv(testing_data_path)\n",
        "  if 'PatientID' not in test_data.columns:\n",
        "    raise ValueError('Testing data must contain PatientID column')\n",
        "\n",
        "  patient_ids = test_data['PatientID']\n",
        "  X_test = test_data.drop(columns=['PatientID'])\n",
        "\n",
        "  with gcsfs.GCSFileSystem().open(model_path, 'rb') as f:\n",
        "    model = joblib.load(f)\n",
        "\n",
        "  pd.DataFrame(\n",
        "      {\n",
        "          'PatientID': patient_ids,\n",
        "          'predicted_target': model.predict(X_test)\n",
        "      }\n",
        "  ).to_csv(prediction_dataset_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Zm7GaP6dAbD",
      "metadata": {
        "id": "9Zm7GaP6dAbD"
      },
      "outputs": [],
      "source": [
        "@pipeline(name='healthcare_readmissions_testing_pipeline')\n",
        "def healthcare_readmissions_testing_pipeline(\n",
        "    healthcare_readmissions_dataset_path: str,\n",
        "    scaler_uri: str,\n",
        "    model_uri: str\n",
        "):\n",
        "    load_data_task = loading_data(\n",
        "        input_dataset_path=healthcare_readmissions_dataset_path\n",
        "    )\n",
        "\n",
        "    preprocess_task = preprocess_data(\n",
        "        input_data_path=load_data_task.output\n",
        "    )\n",
        "\n",
        "    normalize_data = normalize_testing_data(\n",
        "        dataset_path=preprocess_task.output,\n",
        "        scaler_path=scaler_uri\n",
        "    )\n",
        "\n",
        "    model_prediction = predicting_model(\n",
        "        testing_data_path=normalize_data.output,\n",
        "        model_path=model_uri\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K2i5CEnXdCHG",
      "metadata": {
        "id": "K2i5CEnXdCHG"
      },
      "outputs": [],
      "source": [
        "from kfp.v2 import compiler\n",
        "\n",
        "# Compile the pipeline\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=healthcare_readmissions_testing_pipeline,\n",
        "    package_path='healthcare_readmissions_testing_pipeline.json'  # This is the output file\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vUg-2zIsdLZ5",
      "metadata": {
        "id": "vUg-2zIsdLZ5"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "pipeline_job = aiplatform.PipelineJob(\n",
        "    display_name='healthcare_readmissions_testing_pipeline',\n",
        "    template_path='healthcare_readmissions_testing_pipeline.json',  # Updated to the correct pipeline file name\n",
        "    pipeline_root=healthcare_readmissions_dataset,\n",
        "    parameter_values={\n",
        "      'healthcare_readmissions_dataset_path': f'{healthcare_readmissions_dataset}/healthcare_readmissions_dataset_test.csv',\n",
        "      'scaler_uri':scaler_uri,\n",
        "      'model_uri': model_uri\n",
        "\n",
        "    },\n",
        "    enable_caching=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CbrQZ5a8dfA_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbrQZ5a8dfA_",
        "outputId": "5390910e-60ce-4059-d1f4-b0b26d94f1a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/healthcare-readmissions-testing-pipeline-20250506234007?project=652395460584\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-testing-pipeline-20250506234007\n"
          ]
        }
      ],
      "source": [
        "pipeline_job.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Testing pipeline",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
