{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4XssXPGeFfmamMoYLqOBlHJ",
      "metadata": {
        "id": "c4XssXPGeFfmamMoYLqOBlHJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade google-cloud-aiplatform kfp\n",
        "from google.cloud import aiplatform\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ECZ4O69H6Hr2",
      "metadata": {
        "id": "ECZ4O69H6Hr2"
      },
      "outputs": [],
      "source": [
        "from kfp.v2.dsl import pipeline\n",
        "from kfp.v2.dsl import component, InputPath, Model, OutputPath, Output, Metrics, Artifact, Dataset\n",
        "from kfp.v2.dsl import pipeline\n",
        "from kfp.v2.dsl import Input\n",
        "from sklearn.metrics import f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FBkSjcNSHLV1",
      "metadata": {
        "id": "FBkSjcNSHLV1"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"numpy\",\"fsspec\", \"gcsfs\"])\n",
        "def load_data(input_dataset_path:str, output_dataset: Output[Dataset]):\n",
        "    import pandas as pd\n",
        "\n",
        "    df = pd.read_csv(input_dataset_path,keep_default_na=False,na_values=[\"\"])\n",
        "    df.to_csv(output_dataset.path, index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "METWWGZXXJGa",
      "metadata": {
        "id": "METWWGZXXJGa"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
        "def split_data(input_data_path: InputPath('Dataset'),\n",
        "               training_data_path: OutputPath('Dataset'),\n",
        "               validation_data_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(input_data_path,keep_default_na=False,na_values=[\"\"])\n",
        "\n",
        "    # Splitting the dataset into training and validation sets\n",
        "    # Assuming the last column is the target\n",
        "    features = df.iloc[:, :-1]\n",
        "    target = df.iloc[:, -1]\n",
        "    X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Combining features and targets for training and validation sets\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "    val_df = pd.concat([X_val, y_val], axis=1)\n",
        "\n",
        "    # Saving the split datasets to the respective output paths\n",
        "    train_df.to_csv(training_data_path, index=False)\n",
        "    val_df.to_csv(validation_data_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fMr6HPHLYwTI",
      "metadata": {
        "id": "fMr6HPHLYwTI"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", 'numpy'])\n",
        "def preprocess_data(input_data_path: InputPath('Dataset'),\n",
        "                    output_data_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    df = pd.read_csv(input_data_path,keep_default_na=False,na_values=[\"\"])\n",
        "\n",
        "    # 1. Handling Missing Values\n",
        "    df['Number of Prior Visits'] = df['Number of Prior Visits'].fillna(df['Number of Prior Visits'].mode()[0])\n",
        "    df['Medications Prescribed'] = df['Medications Prescribed'].fillna(df['Medications Prescribed'].mode()[0])\n",
        "\n",
        "    # Remove age outliers\n",
        "    df = df[df['Age'] <= 100]\n",
        "\n",
        "    # 2. Feature Engineering\n",
        "    exercise_map = {'None': 0, 'Occasional': 1, 'Regular': 2}\n",
        "    df['Exercise_Encoded'] = df['Exercise Frequency'].map(exercise_map)\n",
        "\n",
        "    def bmi_category(bmi):\n",
        "        if bmi < 18.5:\n",
        "            return 'Underweight'\n",
        "        elif bmi < 25:\n",
        "            return 'Normal'\n",
        "        elif bmi < 30:\n",
        "            return 'Overweight'\n",
        "        else:\n",
        "            return 'Obese'\n",
        "    df['BMI_Category'] = df['BMI'].apply(bmi_category)\n",
        "\n",
        "    def age_group(age):\n",
        "        if age < 40:\n",
        "            return '<40'\n",
        "        elif age < 65:\n",
        "            return '40-64'\n",
        "        else:\n",
        "            return '65+'\n",
        "    df['Age_Group'] = df['Age'].apply(age_group)\n",
        "\n",
        "    # One-hot encode\n",
        "    df = pd.get_dummies(df, columns=[\n",
        "        'Gender', 'Ethnicity', 'Diet Type', 'Type of Treatment',\n",
        "        'BMI_Category', 'Age_Group'\n",
        "    ], drop_first=True)\n",
        "\n",
        "    # 3. skewed variable\n",
        "    df['LOS_Log'] = np.log1p(df['Length of Stay'])\n",
        "\n",
        "\n",
        "    # 4. Feature Selection\n",
        "    df = df.drop(columns=[\n",
        "        'Hospital ID', 'Adjusted Weight (kg)', 'Weight (kg)', 'Exercise Frequency', 'Length of Stay'\n",
        "    ])\n",
        "\n",
        "    df.to_csv(output_data_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JLWkcWN9XMIy",
      "metadata": {
        "id": "JLWkcWN9XMIy"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
        "def normalize_training_data(dataset_path: InputPath('Dataset'),\n",
        "                            normalized_dataset_path: OutputPath('Dataset'),\n",
        "                            scaler_artifact: OutputPath('Artifact')):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    X = df.drop(columns=['PatientID', 'Readmission within 30 Days'])\n",
        "    target = df['Readmission within 30 Days']\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Save the scaler to the output path\n",
        "    joblib.dump(scaler, scaler_artifact)\n",
        "\n",
        "    # Combine scaled features with the target into a new DataFrame\n",
        "    df_scaled = pd.DataFrame(features_scaled, columns=X.columns)\n",
        "    df_scaled['Readmission within 30 Days'] = target  # Reattach the target column\n",
        "\n",
        "    # Save the normalized dataset\n",
        "    df_scaled.to_csv(normalized_dataset_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GbDfpNsXXXo8",
      "metadata": {
        "id": "GbDfpNsXXXo8"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\"])\n",
        "def normalize_validation_data(dataset_path: InputPath('Dataset'),\n",
        "                              normalized_dataset_path: OutputPath('Dataset'),\n",
        "                              scaler_artifact: InputPath('Artifact')):\n",
        "    import pandas as pd\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    X = df.drop(columns=['PatientID', 'Readmission within 30 Days'])\n",
        "    target = df['Readmission within 30 Days']\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Save the scaler to the output path\n",
        "    joblib.dump(scaler, scaler_artifact)\n",
        "\n",
        "    # Combine scaled features with the target into a new DataFrame\n",
        "    df_scaled = pd.DataFrame(features_scaled, columns=X.columns)\n",
        "    df_scaled['Readmission within 30 Days'] = target  # Reattach the target column\n",
        "\n",
        "    # Save the normalized dataset\n",
        "    df_scaled.to_csv(normalized_dataset_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U8FbOXBlftlM",
      "metadata": {
        "id": "U8FbOXBlftlM"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"imblearn\"])\n",
        "def smote_augment_data(dataset_path: InputPath('Dataset'),\n",
        "                       output_dataset_path: OutputPath('Dataset')):\n",
        "    import pandas as pd\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    # Load normalized dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Split features and target\n",
        "    X = df.drop(columns=['Readmission within 30 Days'])\n",
        "    y = df['Readmission within 30 Days']\n",
        "\n",
        "    # Apply SMOTE\n",
        "    sm = SMOTE(random_state=42)\n",
        "    X_res, y_res = sm.fit_resample(X, y)\n",
        "\n",
        "    # Combine back into a DataFrame\n",
        "    df_resampled = pd.DataFrame(X_res, columns=X.columns)\n",
        "    df_resampled['Readmission within 30 Days'] = y_res\n",
        "\n",
        "    # Save the new dataset\n",
        "    df_resampled.to_csv(output_dataset_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BhSA-gDvXgRi",
      "metadata": {
        "id": "BhSA-gDvXgRi"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def train_model(training_data_path: InputPath('Dataset'), output_model: Output[Model]):\n",
        "    import pandas as pd\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import joblib\n",
        "\n",
        "    # Load the combined dataset\n",
        "    df = pd.read_csv(training_data_path)\n",
        "    # Assuming the last column is the target\n",
        "    X_train = df.iloc[:, :-1]\n",
        "    y_train = df.iloc[:, -1]\n",
        "\n",
        "    # Create and train the model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "    # Save the trained model to a file\n",
        "    joblib.dump(model, output_model.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eCsv9asXmcm",
      "metadata": {
        "id": "0eCsv9asXmcm"
      },
      "outputs": [],
      "source": [
        "@component(packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
        "def evaluate_model(model: Input[Model], validation_data_path: InputPath('Dataset'), metrics: Output[Metrics]):\n",
        "    import pandas as pd\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score\n",
        "\n",
        "    import joblib\n",
        "\n",
        "    # Load\n",
        "    model = joblib.load(model.path)\n",
        "    val_data = pd.read_csv(validation_data_path)\n",
        "\n",
        "    # Selecting features and target using iloc\n",
        "    X_val_scaled = val_data.iloc[:, :-1]  # All columns except the last one as features\n",
        "    y_val = val_data.iloc[:, -1]  # Last column as target\n",
        "\n",
        "    # Predictions\n",
        "    y_val_pred = model.predict(X_val_scaled)\n",
        "    y_val_pred_proba = model.predict_proba(X_val_scaled)[:, 1]  # Probability of the positive class\n",
        "\n",
        "    # Calculating metrics\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "    class_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
        "    f1 = f1_score(y_val, y_val_pred)\n",
        "    auc = roc_auc_score(y_val, y_val_pred_proba)\n",
        "\n",
        "    # Log the evaluation metrics\n",
        "    metrics.log_metric(\"accuracy\", accuracy)\n",
        "    metrics.log_metric(\"confusion_matrix\", conf_matrix.tolist())\n",
        "    metrics.log_metric(\"classification_report\", class_report)\n",
        "    metrics.log_metric(\"f1_score\", f1)\n",
        "    metrics.log_metric(\"auc\", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hQlcbhszXpKN",
      "metadata": {
        "id": "hQlcbhszXpKN"
      },
      "outputs": [],
      "source": [
        "@pipeline(name='healthcare_readmissions_training_pipeline')\n",
        "def healthcare_readmissions_training_pipeline(healthcare_readmissions_dataset_path:str):\n",
        "\n",
        "    load_data_task = load_data(input_dataset_path = healthcare_readmissions_dataset_path)\n",
        "\n",
        "    preprocess_task = preprocess_data(\n",
        "        input_data_path=load_data_task.output\n",
        "    )\n",
        "\n",
        "    split_result = split_data(input_data_path=preprocess_task.output)\n",
        "\n",
        "    normalize_training = normalize_training_data(\n",
        "        dataset_path=split_result.outputs['training_data_path']\n",
        "    )\n",
        "\n",
        "    # ➕ Apply SMOTE\n",
        "    smote_task = smote_augment_data(\n",
        "        dataset_path=normalize_training.outputs['normalized_dataset_path']\n",
        "    )\n",
        "\n",
        "    normalize_validation = normalize_validation_data(\n",
        "        dataset_path=split_result.outputs['validation_data_path'],\n",
        "        scaler_artifact=normalize_training.outputs['scaler_artifact']\n",
        "    )\n",
        "\n",
        "    trained_model = train_model(training_data_path=smote_task.output)\n",
        "\n",
        "    evaluate_task = evaluate_model(\n",
        "        model=trained_model.output,\n",
        "        validation_data_path=normalize_validation.outputs['normalized_dataset_path']\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9IuClGeiX1ts",
      "metadata": {
        "id": "9IuClGeiX1ts"
      },
      "outputs": [],
      "source": [
        "from kfp.v2 import compiler\n",
        "\n",
        "# Compile the pipeline\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=healthcare_readmissions_training_pipeline,\n",
        "    package_path='healthcare_readmissions_training_pipeline.json'  # This is the output file\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0k-w8j4aX7zt",
      "metadata": {
        "id": "0k-w8j4aX7zt"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "pipeline_job = aiplatform.PipelineJob(\n",
        "    display_name='healthcare_readmissions_training_pipeline',\n",
        "    template_path='healthcare_readmissions_training_pipeline.json',  # Updated to the correct pipeline file name\n",
        "    pipeline_root=healthcare_readmissions_dataset,\n",
        "    parameter_values={\n",
        "      'healthcare_readmissions_dataset_path': f'{healthcare_readmissions_dataset}/healthcare_readmissions_dataset_train.csv'\n",
        "    },\n",
        "    enable_caching=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5hoZ_LfrYLZL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hoZ_LfrYLZL",
        "outputId": "320fc703-40c3-4652-f865-e31cb12c2c72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547')\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
            "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/healthcare-readmissions-training-pipeline-20250506232547?project=652395460584\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547 current state:\n",
            "PipelineState.PIPELINE_STATE_RUNNING\n",
            "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/652395460584/locations/us-central1/pipelineJobs/healthcare-readmissions-training-pipeline-20250506232547\n"
          ]
        }
      ],
      "source": [
        "pipeline_job.run()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Training pipeline",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
